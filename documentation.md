I started by downloading the January 2018 data to get a feel for the size and fields in the data.  I set up a quick docker container with mysql on it and created 3 quick temporary tables, figuring I could import the csv data directly into those tables and then be able to get a better sense of the data.  Depending on the amount of data and performance, I would either combine the data in some fashion, or summarize it into a new table.  However, importing the csv data directly into those tables seemed like it was going to take 6+ hours, so I pivoted and decided to write a script to preprocess that data into a smaller set of summarized data.

Understanding that the total csv data (all 3 taxi types, but still only for January 2018) was around 20 million rows, my general goal with the summarize script was to combine that data into fewer rows.  My first iteration attempted to group the data by taxi type, location id, pickup date, and pickup hour.  However, after processing just the yellow data, I could see that this wasn't actually going to result in a smaller enough ratio of rows.  At the time, I had wanted to keep the delination of location id for extra data, but considering that filtering by borough was enough to fit the requirements, I decided to add a mapping to the summarizer to convert location id to borough id